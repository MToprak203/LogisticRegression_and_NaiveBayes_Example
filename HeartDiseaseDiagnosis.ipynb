{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57405648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ef0b74",
   "metadata": {},
   "source": [
    "# Dataset Preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30f731c",
   "metadata": {},
   "source": [
    "We need to transform categorical data into one-hot or dummy encoding form. That provides better results because these allow the representation of categorical data to be more expressive. Many machine learning algorithms cannot work with categorical data directly. The categories must be converted into numbers. This is required for both input and output variables that are categorical. \n",
    "\n",
    "In fact, the categorical data is already numerical but for the sake of curiosity, I want to measure using dummy values how much effective in a situation like this. The reason why I wonder is thinking about that the zero values may not affect the weights of the equation. I mean, for instance, before applying sigmoid function to linear equation (ùúΩ'x), the weigths of the 0 values are lost actually. Of course, I am not sure if it really work that way to classify but I want to see the results and share with you. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c014cee",
   "metadata": {},
   "source": [
    "### Directly Using The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7474f1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reminder: you can use 'ctrl + /' for multicomment lines.\n",
    "\n",
    "df = pd.read_csv(\"heart.csv\")\n",
    "final_df = df\n",
    "final_df = (final_df - np.min(final_df)) / (np.max(final_df) - np.min(final_df)).values\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af43bbf8",
   "metadata": {},
   "source": [
    "### Using One-Hot Encoding  Even For Binary Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e33817f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reminder: you can use 'ctrl + /' for multicomment lines.\n",
    "\n",
    "# df = pd.read_csv(\"heart.csv\")\n",
    "\n",
    "# s_dummies = pd.get_dummies(df['sex'], prefix='sex')\n",
    "# fbs_dummies = pd.get_dummies(df['fbs'], prefix='fbs')\n",
    "# restecg_dummies = pd.get_dummies(df['restecg'], prefix='restecg')\n",
    "# exang_dummies = pd.get_dummies(df['exang'], prefix='exang')\n",
    "# cp_dummies = pd.get_dummies(df['cp'], prefix = \"cp\")\n",
    "# thal_dummies = pd.get_dummies(df['thal'], prefix = \"thal\")\n",
    "# slope_dummies = pd.get_dummies(df['slope'], prefix = \"slope\")\n",
    "\n",
    "# final_df = df.drop(columns = ['sex', 'fbs', 'restecg', 'exang', 'cp', 'thal', 'slope'])\n",
    "# final_df = (final_df - np.min(final_df)) / (np.max(final_df) - np.min(final_df)).values\n",
    "# frames = [final_df, s_dummies, fbs_dummies, restecg_dummies, exang_dummies, cp_dummies, thal_dummies, slope_dummies]\n",
    "# final_df = pd.concat(frames, axis = 1)\n",
    "# final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b731088",
   "metadata": {},
   "source": [
    "Dimension of the dataset is greatly increased. This must be a fair reason not using this method, at least binary categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3b2667",
   "metadata": {},
   "source": [
    "### Using One-Hot Encoding Without Binary Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d950556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reminder: you can use 'ctrl + /' for multicomment lines.\n",
    "\n",
    "# df = pd.read_csv(\"heart.csv\")\n",
    "\n",
    "# cp_dummies = pd.get_dummies(df['cp'], prefix = \"cp\")\n",
    "# thal_dummies = pd.get_dummies(df['thal'], prefix = \"thal\")\n",
    "# slope_dummies = pd.get_dummies(df['slope'], prefix = \"slope\")\n",
    "\n",
    "# final_df = df.drop(columns = ['cp', 'thal', 'slope'])\n",
    "# final_df = (final_df - np.min(final_df)) / (np.max(final_df) - np.min(final_df)).values\n",
    "# frames = [final_df, cp_dummies, thal_dummies, slope_dummies]\n",
    "# final_df = pd.concat(frames, axis = 1)\n",
    "# final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b67bef",
   "metadata": {},
   "source": [
    "### Seperate Dataset Into 80% Train and 20% Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dabd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = final_df.target.values\n",
    "x = final_df.drop(['target'], axis = 1)\n",
    "\n",
    "#Seperate\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state=0)\n",
    "#Transpose\n",
    "x_train = x_train.T\n",
    "y_train = y_train.T\n",
    "x_test = x_test.T\n",
    "y_test = y_test.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59281a3",
   "metadata": {},
   "source": [
    "## Sklearn Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8950691",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(x_train.T,y_train.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b089cc81",
   "metadata": {},
   "source": [
    "## Sklearn Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eec32b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = GaussianNB()\n",
    "nb.fit(x_train.T, y_train.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d4e2d4",
   "metadata": {},
   "source": [
    "# Comparing Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89be2f28",
   "metadata": {},
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "A confusion matrix is a summary of prediction results on a classification problem. The number of correct and incorrect predictions are summarized with count values and broken down by each class. This is the key to the confusion matrix.\n",
    "\n",
    "\n",
    " Reference: https://machinelearningmastery.com/confusion-matrix-machine-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b19a384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall performance of model\n",
    "def accuracy(tn, fp, fn, tp):\n",
    "    return (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "# How accurate the positive predictions are\n",
    "def precision(tp, fp):\n",
    "    return tp / (tp + fp)\n",
    "\n",
    "# Coverage of actual positive sample\n",
    "def recall_sensitivity(tp, fn):\n",
    "    return tp / (tp + fn)\n",
    "\n",
    "# Coverage of actual negative sample\n",
    "def specificity(tn, fp):\n",
    "    return tn / (tn + fp)\n",
    "\n",
    "# Hybrid metric useful for unbalanced classes\n",
    "def f1_score(tp, fp, fn):\n",
    "    return 2 * tp / (2 * tp + fp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1235d1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_head_lr = lr.predict(x_test.T)\n",
    "y_head_nb = nb.predict(x_test.T)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm_lr = confusion_matrix(y_test, y_head_lr)\n",
    "cm_nb = confusion_matrix(y_test, y_head_nb)\n",
    "\n",
    "tn_lr, fp_lr, fn_lr, tp_lr = cm_lr.flatten()\n",
    "tn_nb, fp_nb, fn_nb, tp_nb = cm_nb.flatten()\n",
    "\n",
    "\n",
    "accuracies = {}\n",
    "precisions = {}\n",
    "recall_sensitivities = {}\n",
    "specificities = {}\n",
    "F1_scores = {}\n",
    "\n",
    "accuracies['Logistic_Regression'] = accuracy(tn_lr, fp_lr, fn_lr, tp_lr)\n",
    "precisions['Logistic_Regression'] = precision(tp_lr, fp_lr)\n",
    "recall_sensitivities['Logistic_Regression'] = recall_sensitivity(tp_lr, fn_lr)\n",
    "specificities['Logistic_Regression'] = specificity(tn_lr, fp_lr)\n",
    "F1_scores['Logistic_Regression'] = f1_score(tp_lr, fp_lr, fn_lr)\n",
    "\n",
    "accuracies['Naive_Bayes'] = accuracy(tn_nb, fp_nb, fn_nb, tp_nb)\n",
    "precisions['Naive_Bayes'] = precision(tp_nb, fp_nb)\n",
    "recall_sensitivities['Naive_Bayes'] = recall_sensitivity(tp_nb, fn_nb)\n",
    "specificities['Naive_Bayes'] = specificity(tn_nb, fp_nb)\n",
    "F1_scores['Naive_Bayes'] = f1_score(tp_nb, fp_nb, fn_nb)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(24,12))\n",
    "\n",
    "plt.suptitle(\"Confusion Matrixes\",fontsize=24, x=0.37)\n",
    "plt.subplots_adjust(wspace = 0.4, hspace= 0.4)\n",
    "\n",
    "plt.subplot(2,3,1)\n",
    "plt.title(\"Logistic Regression Confusion Matrix\")\n",
    "sns.heatmap(cm_lr,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n",
    "\n",
    "plt.subplot(2,3,2)\n",
    "plt.title(\"Naive Bayes Confusion Matrix\")\n",
    "sns.heatmap(cm_nb,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2ea1b0",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b1b457",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Logistic Regression: ' + str(accuracies['Logistic_Regression']*100))\n",
    "print('Naive Bayes: ' + str(accuracies['Naive_Bayes']*100))\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.yticks(np.arange(0,100,10))\n",
    "plt.ylabel(\"Accuracy %\")\n",
    "plt.xlabel(\"Algorithms\")\n",
    "sns.barplot(x=list(accuracies.keys()), y=list(accuracies.values()))\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(24,5))\n",
    "\n",
    "plt.suptitle(\"Accuracy Comparison\",fontsize=24, x=0.37)\n",
    "plt.subplots_adjust(wspace = 0.4, hspace= 0.4)\n",
    "\n",
    "plt.yticks(np.arange(0,100,10))\n",
    "plt.ylabel(\"Accuracy %\")\n",
    "plt.xlabel(\"Algorithms\")\n",
    "\n",
    "plt.subplot(2,3,1)\n",
    "plt.title(\"Logistic Regression \")\n",
    "sns.barplot(x=['Directly', 'With Binary', 'Without Binary'], y=[83.61, 86.89, 86.89])\n",
    "\n",
    "plt.subplot(2,3,2)\n",
    "plt.title(\"Naive Bayes\")\n",
    "sns.barplot(x=['Directly', 'With Binary', 'Without Binary'], y=[85.25, 85.25, 86.89])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e14a25",
   "metadata": {},
   "source": [
    "Directly Using The Dataset: \n",
    "    \n",
    "    Sklearn Logistic Regression Accuracy: 83.61%\n",
    "    Sklearn Naive Bayes Accuracy: 85.25%\n",
    "\n",
    "Using Dummy Variables Even For Binary Categories:\n",
    "    \n",
    "    Sklearn Logistic Regression Accuracy: 86.89%\n",
    "    Sklearn Naive Bayes Accuracy: 85.25%\n",
    "\n",
    "Using Dummy Variables Without Binary Categories:\n",
    "    \n",
    "    Sklearn Logistic Regression Accuracy: 86.89%\n",
    "    Sklearn Naive Bayes Accuracy: 86.89%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a2b62c",
   "metadata": {},
   "source": [
    "## Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b30c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Logistic Regression: ' + str(precisions['Logistic_Regression'] * 100))\n",
    "print('Naive Bayes: ' + str(precisions['Naive_Bayes'] * 100))\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.yticks(np.arange(0,100,10))\n",
    "plt.ylabel(\"Precision %\")\n",
    "plt.xlabel(\"Algorithms\")\n",
    "sns.barplot(x=list(precisions.keys()), y=list(precisions.values()))\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(24,5))\n",
    "\n",
    "plt.suptitle(\"Precision Comparison\",fontsize=24, x=0.37)\n",
    "plt.subplots_adjust(wspace = 0.4, hspace= 0.4)\n",
    "\n",
    "plt.yticks(np.arange(0,100,10))\n",
    "plt.ylabel(\"Precision %\")\n",
    "plt.xlabel(\"Algorithms\")\n",
    "\n",
    "plt.subplot(2,3,1)\n",
    "plt.title(\"Logistic Regression\")\n",
    "sns.barplot(x=['Directly', 'With Binary', 'Without Binary'], y=[81.58, 88.24, 88.24])\n",
    "\n",
    "plt.subplot(2,3,2)\n",
    "plt.title(\"Naive Bayes\")\n",
    "sns.barplot(x=['Directly', 'With Binary', 'Without Binary'], y=[83.78, 85.71, 88.24])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d643b342",
   "metadata": {},
   "source": [
    "Directly Using The Dataset: \n",
    "    \n",
    "    Sklearn Logistic Regression Precision: 81.58%\n",
    "    Sklearn Naive Bayes Precision: 83.78%\n",
    "\n",
    "Using Dummy Variables Even For Binary Categories:\n",
    "    \n",
    "    Sklearn Logistic Regression Precision: 88.24%\n",
    "    Sklearn Naive Bayes Precision: 85.71%\n",
    "\n",
    "Using Dummy Variables Without Binary Categories:\n",
    "    \n",
    "    Sklearn Logistic Regression Precision: 88.24%\n",
    "    Sklearn Naive Bayes Precision: 88.24%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59113c95",
   "metadata": {},
   "source": [
    "## Recall Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f9aef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Logistic Regression: ' + str(recall_sensitivities['Logistic_Regression']*100))\n",
    "print('Naive Bayes: ' + str(recall_sensitivities['Naive_Bayes']*100))\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.yticks(np.arange(0,100,10))\n",
    "plt.ylabel(\"Recall Sensitivities\")\n",
    "plt.xlabel(\"Algorithms\")\n",
    "sns.barplot(x=list(recall_sensitivities.keys()), y=list(recall_sensitivities.values()))\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(24,5))\n",
    "\n",
    "plt.suptitle(\"Recall Sensitivities Comparison\",fontsize=24, x=0.37)\n",
    "plt.subplots_adjust(wspace = 0.4, hspace= 0.4)\n",
    "\n",
    "plt.yticks(np.arange(0,100,10))\n",
    "plt.ylabel(\"Recall Sensitivities %\")\n",
    "plt.xlabel(\"Algorithms\")\n",
    "\n",
    "plt.subplot(2,3,1)\n",
    "plt.title(\"Logistic Regression \")\n",
    "sns.barplot(x=['Directly', 'With Binary', 'Without Binary'], y=[91.18, 88.24, 88.24])\n",
    "\n",
    "plt.subplot(2,3,2)\n",
    "plt.title(\"Naive Bayes\")\n",
    "sns.barplot(x=['Directly', 'With Binary', 'Without Binary'], y=[91.18, 88.24, 88.24])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884b8350",
   "metadata": {},
   "source": [
    "Directly Using The Dataset: \n",
    "    \n",
    "    Sklearn Logistic Regression Recall Sensitivity: 91.18%\n",
    "    Sklearn Naive Bayes Recall Sensitivity: 91.18%\n",
    "\n",
    "Using Dummy Variables Even For Binary Categories:\n",
    "    \n",
    "    Sklearn Logistic Regression Recall Sensitivity: 88.24%\n",
    "    Sklearn Naive Bayes Recall Sensitivity: 88.24%\n",
    "\n",
    "Using Dummy Variables Without Binary Categories:\n",
    "    \n",
    "    Sklearn Logistic Regression Recall Sensitivity: 88.24%\n",
    "    Sklearn Naive Bayes Recall Sensitivity: 88.24%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f698a74",
   "metadata": {},
   "source": [
    "## Specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cee9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Logistic Regression: ' + str(specificities['Logistic_Regression']*100))\n",
    "print('Naive Bayes: ' + str(specificities['Naive_Bayes']*100))\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.yticks(np.arange(0,100,10))\n",
    "plt.ylabel(\"Specificities\")\n",
    "plt.xlabel(\"Algorithms\")\n",
    "sns.barplot(x=list(specificities.keys()), y=list(specificities.values()))\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(24,5))\n",
    "\n",
    "plt.suptitle(\"Specificity Comparison\",fontsize=24, x=0.37)\n",
    "plt.subplots_adjust(wspace = 0.4, hspace= 0.4)\n",
    "\n",
    "plt.yticks(np.arange(0,100,10))\n",
    "plt.ylabel(\"Specificity %\")\n",
    "plt.xlabel(\"Algorithms\")\n",
    "\n",
    "plt.subplot(2,3,1)\n",
    "plt.title(\"Logistic Regression\")\n",
    "sns.barplot(x=['Directly', 'With Binary', 'Without Binary'], y=[74.07, 85.19, 85.19])\n",
    "\n",
    "plt.subplot(2,3,2)\n",
    "plt.title(\"Naive Bayes\")\n",
    "sns.barplot(x=['Directly', 'With Binary', 'Without Binary'], y=[77.78, 81.48, 85.19])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ff891d",
   "metadata": {},
   "source": [
    "Directly Using The Dataset: \n",
    "    \n",
    "    Sklearn Logistic Regression Specifificity: 74.07%\n",
    "    Sklearn Naive Bayes Specifificity: 77.78%\n",
    "\n",
    "Using Dummy Variables Even For Binary Categories:\n",
    "    \n",
    "    Sklearn Logistic Regression Specifificity: 85.19%\n",
    "    Sklearn Naive Bayes Specifificity: 81.48%\n",
    "\n",
    "Using Dummy Variables Without Binary Categories:\n",
    "    \n",
    "    Sklearn Logistic Regression Specifificity: 85.19%\n",
    "    Sklearn Naive Bayes Specifificity: 85.19%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578d305b",
   "metadata": {},
   "source": [
    "## F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67601e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Logistic Regression: ' + str(F1_scores['Logistic_Regression']*100))\n",
    "print('Naive Bayes: ' + str(F1_scores['Naive_Bayes']*100))\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.yticks(np.arange(0,100,10))\n",
    "plt.ylabel(\"F1 Scores\")\n",
    "plt.xlabel(\"Algorithms\")\n",
    "sns.barplot(x=list(F1_scores.keys()), y=list(F1_scores.values()))\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(24,5))\n",
    "\n",
    "plt.suptitle(\"F1 Score Comparison\",fontsize=24, x=0.37)\n",
    "plt.subplots_adjust(wspace = 0.4, hspace= 0.4)\n",
    "\n",
    "plt.yticks(np.arange(0,100,10))\n",
    "plt.ylabel(\"F1 Score %\")\n",
    "plt.xlabel(\"Algorithms\")\n",
    "\n",
    "plt.subplot(2,3,1)\n",
    "plt.title(\"Logistic Regression\")\n",
    "sns.barplot(x=['Directly', 'With Binary', 'Without Binary'], y=[86.11, 88.24, 88.24])\n",
    "\n",
    "plt.subplot(2,3,2)\n",
    "plt.title(\"Naive Bayes\")\n",
    "sns.barplot(x=['Directly', 'With Binary', 'Without Binary'], y=[87.32, 86.96, 88.24])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6db95fc",
   "metadata": {},
   "source": [
    "Directly Using The Dataset: \n",
    "    \n",
    "    Sklearn Logistic Regression F1 Score: 86.11%\n",
    "    Sklearn Naive Bayes F1 Score: 87.32%\n",
    "\n",
    "Using Dummy Variables Even For Binary Categories:\n",
    "    \n",
    "    Sklearn Logistic Regression F1 Score: 88.24%\n",
    "    Sklearn Naive Bayes F1 Score: 86.96%\n",
    "\n",
    "Using Dummy Variables Without Binary Categories:\n",
    "    \n",
    "    Sklearn Logistic Regression F1 Score: 88.24%\n",
    "    Sklearn Naive Bayes F1 Score: 88.24%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
